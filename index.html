<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Generative Image Layer Decomposition with Visual Effects</title>


  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./resources/icon.png"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Generative Image Layer Decomposition with Visual Effects</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <span>Jinrui Yang</a></span><sup>*1,2</sup>,</span>
            <span class="author-block">
              <span>Qing Liu</a></span><sup>2</sup>,</span>
            <span class="author-block">
              <span>Yijun Li</a></span><sup>2</sup>,</span>
            </span>
            <span class="author-block">
              <span>Soo Ye Kim</a></span><sup>2</sup>,</span>
            </span>
            <span class="author-block">
              <span>Daniil Pakhomov</a></span><sup>2</sup>,</span>
            </span>
            <span class="author-block">
              <span>Mengwei Ren</a></span><sup>2</sup>,</span>
            </span>
            <span class="author-block">
              <span>Jianming Zhang</a></span><sup>2</sup>,</span>
            </span>
            <span class="author-block">
              <span>Zhe Lin</a></span><sup>2</sup>,</span>
            </span>
            <span class="author-block">
              <span>Cihang Xie</a></span><sup>1</sup>,</span>
            </span>
            <span class="author-block">
              <span>Yuyin Zhou</a></span><sup>1</sup>,</span>
            </span>
          </div>
          
          <!-- Equal contribution note -->
          <div class="equal-contribution-advising-note">
            <p>* equal contribution, † equal advising</p>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <span><sup>1</sup>UC Santa Cruz</a></span>,</span>
            <span class="author-block">
              <span><sup>2</sup>Adobe Research</a></span></span>
            <!-- <span class="author-block"><sup>3</sup>UC, Santa Cruz</span> -->
            
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2405.20299"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa-solidassasas fa-face-smiling-hands"></i>
                    <img src="./resources/ar.svg" alt="img" style="width: 100%; height: 100%" /> 
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/UCSC-VLAA/CRATE-alpha"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              
              <!-- Model Link. -->
              <!-- <span class="link-block">
                <a href="https://huggingface.co/UCSC-VLAA/CRATE-alpha/tree/main"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa-solidassasas fa-face-smiling-hands"></i>
                      <img src="./resources/hg.svg" alt="img" style="width: 100%; height: 100%" /> 
                  </span>
                  <span>Model</span>
                  </a>
              </span> -->
              

            </div>

          </div>


        </div>
      </div>
    </div>
  </div>
</section>

<br>

<section class="hero teaser">
  <div class="container">
    <div class="hero-body">
      <center><h2 class="title is-3">Overview of LayerDecomp</h2></center>
      <center><img src="./resources/teaser_v2.pdf" alt="alt text"
                        style="width: 80%; object-fit: cover; max-width:80%;"></a></center>
      <h2 class="subtitle has-text-centered">
        (a) Given an input image and a binary object mask, our model is able to decompose the image into a clean background layer and a transparent foreground layer with preserved visual effects such as shadows and reflections. (b) Subsequently, our decomposition empowers complex and controllable layer-wise editing such as spatial, color and/or style editing.
      </h2>
    </div>
  </div>
</section>

<br>

<section class="section">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in large generative models, particularly diffusion-based methods, have significantly enhanced the capabilities of image editing. However, achieving precise control over image composition tasks remains a challenge.  Layered representations, which allow for independent editing of image components, are essential for user-driven content creation, yet existing approaches often struggle to decompose image into plausible layers with accurately retained transparent visual effects such as shadows and reflections. We propose <b>LayerDecomp</b>, a generative framework for image layer decomposition which outputs photorealistic clean backgrounds and high-quality transparent foregrounds with faithfully preserved visual effects. To enable effective training, we first introduce a dataset preparation pipeline that automatically scales up simulated multi-layer data with synthesized visual effects. To further enhance real-world applicability, we supplement this simulated dataset with camera-captured images containing natural visual effects.  Additionally, we propose a consistency loss which enforces the model to learn accurate representations for the transparent foreground layer when ground-truth annotations are not available. Our method achieves superior quality in layer decomposition, outperforming existing approaches in object removal and spatial editing tasks across several benchmarks and multiple user studies, unlocking various creative possibilities for layer-wise image editing. The project page is https://rayjryang.github.io/LayerDecomp/.
          </p>
        </div>
      </div>
    </div>
  </section>

<br>

<section class="hero teaser">
  <div class="container">
    <div class="hero-body">
      <center><h2 class="title is-3">Comparison of CRATE, CRATE-α, and ViT </h2></center>
      <center><img src="./resources/fig_1_crate_alpha.png" alt="alt text"
                        style="width: 80%; object-fit: cover; max-width:80%;"></a></center>
      <h2 class="subtitle has-text-centered">
        <i>Left:</i> We demonstrate how modifications to the components enhance the performance of the <b>CRATE</b> model on ImageNet-1K. <i>Right:</i> We compare the FLOPs and accuracy on ImageNet-1K of our methods with ViT <a href="https://arxiv.org/abs/2010.11929">Dosovitskiy et al., 2020</a> and CRATE <a href="https://ma-lab-berkeley.github.io/CRATE/">Yu et al., 2023</a>. CRATE is trained only on ImageNet-1K, while <b>ours</b> and ViT are pre-trained on ImageNet-21K.
      </h2>
    </div>
  </div>
</section>

<br>

<section class="hero teaser">
  <div class="container">
    <div class="hero-body">
      <center><h2 class="title is-3">Visualize the Improvement of Semantic Interpretability of CRATE-α</h2></center>
      <center><img src="./resources/figure_cutler_segmentation.png" alt="alt text"
                        style="width: 80%; object-fit: cover; max-width:80%;"></a></center>
      <h2 class="subtitle has-text-centered">
        <strong>Visualization of segmentation on COCO val2017 <a href="https://arxiv.org/abs/1405.0312">Lin et al., 2014</a> with MaskCut <a href="https://arxiv.org/abs/2301.11320">Wang et al., 2023</a>.</strong>  
        <em>Top row</em>: Supervised <strong>ours</strong> effectively identifies the main objects in the image. Compared with <strong>CRATE</strong> (<em>Middle row</em>), <strong>ours</strong> achieves better segmentation performance in terms of boundary.
        <em>Bottom row</em>: Supervised ViT fails to identify the main objects in most images. We mark failed images with <img src="./resources/red_box.png" alt="Red Box" style="width: 0.25cm;">.
      </h2>
    </div>
  </div>
</section>


<br>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Based on the following <a href="http://nerfies.github.io">template</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>